#!/usr/bin/env python3
"""
å›¾ç‰‡æ–‡ä»¶ç³»ç»Ÿåˆ†æè„šæœ¬
ç”¨äºåˆ†ædownloaded_imagesç›®å½•ä¸­çš„å›¾ç‰‡æ–‡ä»¶æƒ…å†µ
"""

import os
import glob
import hashlib
import json
from pathlib import Path
import pandas as pd
from collections import Counter, defaultdict

def format_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°ä¸ºäººç±»å¯è¯»çš„æ ¼å¼"""
    if size_bytes == 0:
        return "0 B"
    size_names = ["B", "KB", "MB", "GB", "TB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    return f"{size_bytes:.1f} {size_names[i]}"

def calculate_file_hash(file_path):
    """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        return None

def analyze_duplicate_files(image_files):
    """åˆ†æé‡å¤æ–‡ä»¶ï¼ˆåŸºäºæ–‡ä»¶å“ˆå¸Œï¼‰"""
    print("ğŸ”„ é‡å¤æ–‡ä»¶åˆ†æ:")
    print("-" * 30)
    
    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
    file_hashes = {}
    hash_to_files = defaultdict(list)
    
    print("  æ­£åœ¨è®¡ç®—æ–‡ä»¶å“ˆå¸Œ...")
    for i, file_path in enumerate(image_files):
        if i % 1000 == 0:
            print(f"    å·²å¤„ç† {i:,} / {len(image_files):,} ä¸ªæ–‡ä»¶")
        
        file_hash = calculate_file_hash(file_path)
        if file_hash:
            file_hashes[file_path] = file_hash
            hash_to_files[file_hash].append(file_path)
    
    # æ‰¾å‡ºé‡å¤æ–‡ä»¶
    duplicate_groups = {hash_val: files for hash_val, files in hash_to_files.items() if len(files) > 1}
    
    print(f"  é‡å¤æ–‡ä»¶ç»„æ•°: {len(duplicate_groups):,}")
    print(f"  æ¶‰åŠæ–‡ä»¶æ•°: {sum(len(files) for files in duplicate_groups.values()):,}")
    
    if duplicate_groups:
        print("  é‡å¤æ–‡ä»¶ç»„ï¼ˆå‰5ç»„ï¼‰:")
        for i, (hash_val, files) in enumerate(list(duplicate_groups.items())[:5]):
            print(f"    ç»„ {i+1}:")
            for file_path in files:
                print(f"      - {file_path.name} ({format_size(file_path.stat().st_size)})")
    
    return duplicate_groups, hash_to_files

def generate_json_report(analysis_data):
    """ç”ŸæˆJSONæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š"""
    json_report = {
        "analysis_summary": {
            "total_files": analysis_data["total_files"],
            "total_size_bytes": analysis_data["total_size"],
            "total_size_human": format_size(analysis_data["total_size"]),
            "average_size_bytes": analysis_data["avg_size"],
            "average_size_human": format_size(analysis_data["avg_size"]),
            "analysis_timestamp": analysis_data["timestamp"]
        },
        "file_type_distribution": analysis_data["ext_counter"],
        "size_distribution": analysis_data["size_distribution"],
        "file_name_analysis": {
            "name_length_range": analysis_data["name_length_range"],
            "common_name_lengths": analysis_data["common_name_lengths"],
            "numeric_names_count": analysis_data["numeric_names_count"],
            "numeric_names_percentage": analysis_data["numeric_names_percentage"]
        },
        "directory_structure": {
            "subdirectory_count": analysis_data["subdir_count"],
            "subdirectories": analysis_data["subdirs_list"]
        },
        "anomaly_checks": {
            "empty_files": {
                "count": analysis_data["empty_files_count"],
                "files": analysis_data["empty_files_list"]
            },
            "small_files": analysis_data["small_files_count"],
            "large_files": analysis_data["large_files_count"],
            "non_standard_extensions": analysis_data["non_standard_files_count"]
        },
        "duplicate_files": {
            "duplicate_groups_count": analysis_data["duplicate_groups_count"],
            "files_involved_count": analysis_data["files_involved_count"],
            "duplicate_groups": analysis_data["duplicate_groups_details"]
        }
    }
    
    # ä¿å­˜JSONæŠ¥å‘Š
    json_file = "detailed_image_analysis.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(json_report, f, ensure_ascii=False, indent=2)
    
    print(f"  JSONè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {json_file}")
    return json_report

def generate_markdown_report(analysis_data, json_report):
    """ç”ŸæˆMarkdownæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š"""
    md_content = f"""# å›¾ç‰‡æ–‡ä»¶ç³»ç»Ÿè¯¦ç»†åˆ†ææŠ¥å‘Š

## ğŸ“Š æ€»ä½“æ¦‚å†µ

- **åˆ†ææ—¶é—´**: {analysis_data["timestamp"]}
- **ç›®å½•ä½ç½®**: {analysis_data["directory_path"]}
- **æ–‡ä»¶æ€»æ•°**: {analysis_data["total_files"]:,}
- **æ€»å¤§å°**: {format_size(analysis_data["total_size"])}
- **å¹³å‡æ–‡ä»¶å¤§å°**: {format_size(analysis_data["avg_size"])}

## ğŸ“„ æ–‡ä»¶ç±»å‹åˆ†å¸ƒ

| æ–‡ä»¶ç±»å‹ | æ•°é‡ | ç™¾åˆ†æ¯” |
|---------|------|--------|
"""
    
    # æ·»åŠ æ–‡ä»¶ç±»å‹è¡¨æ ¼
    for ext, count in sorted(analysis_data["ext_counter"].items(), key=lambda x: x[1], reverse=True):
        percentage = (count / analysis_data["total_files"]) * 100
        md_content += f"| {ext or 'æ— æ‰©å±•å'} | {count:,} | {percentage:.1f}% |\n"
    
    md_content += f"""
## ğŸ“ æ–‡ä»¶å¤§å°åˆ†æ

- **æœ€å°æ–‡ä»¶**: {format_size(analysis_data["min_size"])}
- **æœ€å¤§æ–‡ä»¶**: {format_size(analysis_data["max_size"])}

### å¤§å°åˆ†å¸ƒ

| å¤§å°èŒƒå›´ | æ–‡ä»¶æ•°é‡ | ç™¾åˆ†æ¯” |
|---------|----------|--------|
"""
    
    # æ·»åŠ å¤§å°åˆ†å¸ƒè¡¨æ ¼
    for size_range in analysis_data["size_distribution"]:
        md_content += f"| {size_range['label']} | {size_range['count']:,} | {size_range['percentage']:.1f}% |\n"
    
    md_content += f"""
## ğŸ·ï¸ æ–‡ä»¶åæ¨¡å¼åˆ†æ

- **æ–‡ä»¶åé•¿åº¦èŒƒå›´**: {analysis_data["name_length_range"][0]} - {analysis_data["name_length_range"][1]} å­—ç¬¦
- **æ•°å­—æ–‡ä»¶å**: {analysis_data["numeric_names_count"]:,} ä¸ª ({analysis_data["numeric_names_percentage"]:.1f}%)

### å¸¸è§æ–‡ä»¶åé•¿åº¦

| é•¿åº¦ | æ–‡ä»¶æ•°é‡ | ç™¾åˆ†æ¯” |
|------|----------|--------|
"""
    
    # æ·»åŠ æ–‡ä»¶åé•¿åº¦è¡¨æ ¼
    for length_info in analysis_data["common_name_lengths"]:
        md_content += f"| {length_info['length']} å­—ç¬¦ | {length_info['count']:,} | {length_info['percentage']:.1f}% |\n"
    
    md_content += f"""
## ğŸ“‚ ç›®å½•ç»“æ„åˆ†æ

- **å­ç›®å½•æ•°é‡**: {analysis_data["subdir_count"]}
"""
    
    if analysis_data["subdirs_list"]:
        md_content += "\n### å­ç›®å½•åˆ—è¡¨\n\n"
        for subdir in analysis_data["subdirs_list"]:
            md_content += f"- {subdir['name']} ({subdir['file_count']} ä¸ªæ–‡ä»¶)\n"
    else:
        md_content += "\næ‰€æœ‰æ–‡ä»¶éƒ½åœ¨æ ¹ç›®å½•\n"
    
    md_content += f"""
## âš ï¸ å¼‚å¸¸æ–‡ä»¶æ£€æŸ¥

### ç©ºæ–‡ä»¶
- **æ•°é‡**: {analysis_data["empty_files_count"]:,}
"""
    
    if analysis_data["empty_files_list"]:
        md_content += "\n**ç©ºæ–‡ä»¶åˆ—è¡¨**:\n\n"
        for file in analysis_data["empty_files_list"]:
            md_content += f"- {file}\n"
    
    md_content += f"""
### å…¶ä»–å¼‚å¸¸
- **æå°æ–‡ä»¶ (<100B)**: {analysis_data["small_files_count"]:,}
- **è¶…å¤§æ–‡ä»¶ (>10MB)**: {analysis_data["large_files_count"]:,}
- **éæ ‡å‡†æ‰©å±•å**: {analysis_data["non_standard_files_count"]:,}

## ğŸ”„ é‡å¤æ–‡ä»¶åˆ†æ

- **é‡å¤æ–‡ä»¶ç»„æ•°**: {analysis_data["duplicate_groups_count"]:,}
- **æ¶‰åŠæ–‡ä»¶æ•°**: {analysis_data["files_involved_count"]:,}

### é‡å¤æ–‡ä»¶ç»„è¯¦æƒ…ï¼ˆå‰10ç»„ï¼‰

"""
    
    # æ·»åŠ é‡å¤æ–‡ä»¶ç»„ä¿¡æ¯
    for i, group in enumerate(analysis_data["duplicate_groups_details"][:10]):
        md_content += f"#### ç»„ {i+1}\n\n"
        for file_info in group["files"]:
            md_content += f"- {file_info['name']} ({file_info['size']})\n"
        md_content += "\n"
    
    md_content += f"""
## ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡

- **å”¯ä¸€æ‰©å±•åæ•°é‡**: {analysis_data["unique_extensions"]}
- **æ•°å­—æ–‡ä»¶åæ¯”ä¾‹**: {analysis_data["numeric_names_percentage"]:.1f}%

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {analysis_data["timestamp"]}*
*æ•°æ®æ¥æº: {analysis_data["directory_path"]}*
"""
    
    # ä¿å­˜MarkdownæŠ¥å‘Š
    md_file = "detailed_image_analysis.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"  Markdownè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {md_file}")
    return md_content

def analyze_images_directory():
    """åˆ†ædownloaded_imagesç›®å½•çš„æ–‡ä»¶ç³»ç»Ÿæƒ…å†µ"""
    
    import datetime
    
    image_dir = Path("downloaded_images")
    
    if not image_dir.exists():
        print(f"é”™è¯¯ï¼šç›®å½• {image_dir} ä¸å­˜åœ¨")
        return
    
    print("=" * 60)
    print("å›¾ç‰‡æ–‡ä»¶ç³»ç»Ÿåˆ†ææŠ¥å‘Š")
    print("=" * 60)
    
    # è·å–æ‰€æœ‰æ–‡ä»¶
    all_files = list(image_dir.rglob("*"))
    image_files = [f for f in all_files if f.is_file()]
    
    print(f"ğŸ“ ç›®å½•ä½ç½®: {image_dir.absolute()}")
    print(f"ğŸ“Š æ–‡ä»¶æ€»æ•°: {len(image_files):,}")
    print()
    
    # æ”¶é›†æ‰€æœ‰åˆ†ææ•°æ®
    analysis_data = {
        "directory_path": str(image_dir.absolute()),
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "total_files": len(image_files)
    }
    
    # 1. æ–‡ä»¶ç±»å‹åˆ†æ
    print("ğŸ“„ æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
    print("-" * 30)
    extensions = [f.suffix.lower() for f in image_files]
    ext_counter = Counter(extensions)
    analysis_data["ext_counter"] = dict(ext_counter.most_common())
    analysis_data["unique_extensions"] = len(ext_counter)
    
    for ext, count in ext_counter.most_common():
        percentage = (count / len(image_files)) * 100
        print(f"  {ext or 'æ— æ‰©å±•å':<8} {count:>6,} ä¸ª ({percentage:>5.1f}%)")
    print()
    
    # 2. æ–‡ä»¶å¤§å°åˆ†æ
    print("ğŸ“ æ–‡ä»¶å¤§å°åˆ†æ:")
    print("-" * 30)
    sizes = [f.stat().st_size for f in image_files]
    
    if sizes:
        total_size = sum(sizes)
        avg_size = total_size / len(sizes)
        min_size = min(sizes)
        max_size = max(sizes)
        
        analysis_data["total_size"] = total_size
        analysis_data["avg_size"] = avg_size
        analysis_data["min_size"] = min_size
        analysis_data["max_size"] = max_size
        
        print(f"  æ€»å¤§å°: {format_size(total_size)}")
        print(f"  å¹³å‡å¤§å°: {format_size(avg_size)}")
        print(f"  æœ€å°æ–‡ä»¶: {format_size(min_size)}")
        print(f"  æœ€å¤§æ–‡ä»¶: {format_size(max_size)}")
        print()
        
        # å¤§å°åˆ†å¸ƒ
        size_ranges = [
            (0, 1024, "0-1KB"),
            (1024, 10240, "1-10KB"),
            (10240, 102400, "10-100KB"),
            (102400, 1048576, "100KB-1MB"),
            (1048576, float('inf'), ">1MB")
        ]
        
        size_distribution = []
        print("  å¤§å°åˆ†å¸ƒ:")
        for min_r, max_r, label in size_ranges:
            count = sum(1 for s in sizes if min_r <= s < max_r)
            percentage = (count / len(sizes)) * 100
            size_distribution.append({
                "label": label,
                "count": count,
                "percentage": percentage
            })
            print(f"    {label:<12} {count:>6,} ä¸ª ({percentage:>5.1f}%)")
        
        analysis_data["size_distribution"] = size_distribution
    print()
    
    # 3. ç©ºæ–‡ä»¶æ£€æŸ¥
    print("ğŸ” ç©ºæ–‡ä»¶æ£€æŸ¥:")
    print("-" * 30)
    empty_files = [f for f in image_files if f.stat().st_size == 0]
    analysis_data["empty_files_count"] = len(empty_files)
    analysis_data["empty_files_list"] = [str(f.relative_to(image_dir)) for f in empty_files]
    
    print(f"  ç©ºæ–‡ä»¶æ•°é‡: {len(empty_files):,}")
    if empty_files:
        print("  ç©ºæ–‡ä»¶åˆ—è¡¨:")
        for f in empty_files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"    - {f.name}")
        if len(empty_files) > 10:
            print(f"    ... è¿˜æœ‰ {len(empty_files) - 10} ä¸ªç©ºæ–‡ä»¶")
    print()
    
    # 4. æ–‡ä»¶åæ¨¡å¼åˆ†æ
    print("ğŸ·ï¸  æ–‡ä»¶åæ¨¡å¼åˆ†æ:")
    print("-" * 30)
    filenames = [f.stem for f in image_files]  # å»æ‰æ‰©å±•åçš„æ–‡ä»¶å
    
    # åˆ†ææ–‡ä»¶åé•¿åº¦åˆ†å¸ƒ
    name_lengths = [len(name) for name in filenames]
    if name_lengths:
        name_length_range = (min(name_lengths), max(name_lengths))
        analysis_data["name_length_range"] = name_length_range
        print(f"  æ–‡ä»¶åé•¿åº¦èŒƒå›´: {min(name_lengths)} - {max(name_lengths)} å­—ç¬¦")
        
        # ç»Ÿè®¡å¸¸è§é•¿åº¦
        length_counter = Counter(name_lengths)
        common_name_lengths = []
        print("  å¸¸è§æ–‡ä»¶åé•¿åº¦:")
        for length, count in length_counter.most_common(5):
            percentage = (count / len(filenames)) * 100
            common_name_lengths.append({
                "length": length,
                "count": count,
                "percentage": percentage
            })
            print(f"    {length} å­—ç¬¦: {count:>6,} ä¸ª ({percentage:>5.1f}%)")
        
        analysis_data["common_name_lengths"] = common_name_lengths
    
    # æ£€æŸ¥æ˜¯å¦éƒ½æ˜¯æ•°å­—ï¼ˆæ¡ç ï¼‰
    numeric_names = [name for name in filenames if name.isdigit()]
    numeric_names_count = len(numeric_names)
    numeric_names_percentage = (numeric_names_count / len(filenames)) * 100
    analysis_data["numeric_names_count"] = numeric_names_count
    analysis_data["numeric_names_percentage"] = numeric_names_percentage
    
    print(f"  æ•°å­—æ–‡ä»¶å: {numeric_names_count:,} ä¸ª ({numeric_names_percentage:.1f}%)")
    print()
    
    # 5. ç›®å½•ç»“æ„åˆ†æ
    print("ğŸ“‚ ç›®å½•ç»“æ„åˆ†æ:")
    print("-" * 30)
    subdirs = [d for d in all_files if d.is_dir()]
    analysis_data["subdir_count"] = len(subdirs)
    
    subdirs_list = []
    for d in subdirs:
        files_in_dir = len([f for f in d.rglob("*") if f.is_file()])
        subdirs_list.append({
            "name": str(d.relative_to(image_dir)),
            "file_count": files_in_dir
        })
    analysis_data["subdirs_list"] = subdirs_list
    
    print(f"  å­ç›®å½•æ•°é‡: {len(subdirs)}")
    
    if subdirs:
        print("  å­ç›®å½•åˆ—è¡¨:")
        for d in subdirs:
            files_in_dir = len([f for f in d.rglob("*") if f.is_file()])
            print(f"    - {d.relative_to(image_dir)} ({files_in_dir} ä¸ªæ–‡ä»¶)")
    else:
        print("  æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨æ ¹ç›®å½•")
    print()
    
    # 6. å¼‚å¸¸æ–‡ä»¶æ£€æŸ¥
    print("âš ï¸  å¼‚å¸¸æ–‡ä»¶æ£€æŸ¥:")
    print("-" * 30)
    
    # æ£€æŸ¥éå¸¸å°çš„æ–‡ä»¶ï¼ˆå¯èƒ½æŸåï¼‰
    small_files = [f for f in image_files if 0 < f.stat().st_size < 100]
    analysis_data["small_files_count"] = len(small_files)
    print(f"  æå°æ–‡ä»¶ (<100B): {len(small_files):,}")
    
    # æ£€æŸ¥éå¸¸å¤§çš„æ–‡ä»¶
    large_files = [f for f in image_files if f.stat().st_size > 10 * 1024 * 1024]  # >10MB
    analysis_data["large_files_count"] = len(large_files)
    print(f"  è¶…å¤§æ–‡ä»¶ (>10MB): {len(large_files):,}")
    
    # æ£€æŸ¥éæ ‡å‡†æ‰©å±•å
    standard_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}
    non_standard_files = [f for f in image_files if f.suffix.lower() not in standard_extensions and f.suffix]
    analysis_data["non_standard_files_count"] = len(non_standard_files)
    print(f"  éæ ‡å‡†æ‰©å±•å: {len(non_standard_files):,}")
    print()
    
    # 7. é‡å¤æ–‡ä»¶åˆ†æ
    duplicate_groups, hash_to_files = analyze_duplicate_files(image_files)
    analysis_data["duplicate_groups_count"] = len(duplicate_groups)
    analysis_data["files_involved_count"] = sum(len(files) for files in duplicate_groups.values())
    
    # å‡†å¤‡é‡å¤æ–‡ä»¶ç»„è¯¦æƒ…
    duplicate_groups_details = []
    for hash_val, files in list(duplicate_groups.items())[:20]:  # é™åˆ¶å‰20ç»„
        group_files = []
        for file_path in files:
            group_files.append({
                "name": str(file_path.relative_to(image_dir)),
                "size": format_size(file_path.stat().st_size)
            })
        duplicate_groups_details.append({
            "hash": hash_val,
            "files": group_files
        })
    analysis_data["duplicate_groups_details"] = duplicate_groups_details
    print()
    
    # 8. ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡
    print("ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡:")
    print("-" * 30)
    
    # åˆ›å»ºæ•°æ®æ¡†ç”¨äºè¯¦ç»†åˆ†æ
    file_data = []
    for file_path in image_files:
        stat = file_path.stat()
        file_data.append({
            'filename': file_path.name,
            'extension': file_path.suffix.lower(),
            'size_bytes': stat.st_size,
            'size_human': format_size(stat.st_size),
            'is_numeric': file_path.stem.isdigit(),
            'name_length': len(file_path.stem)
        })
    
    df = pd.DataFrame(file_data)
    
    print(f"  æ–‡ä»¶æ•°é‡ç»Ÿè®¡: {len(df):,}")
    print(f"  å”¯ä¸€æ‰©å±•å: {df['extension'].nunique()}")
    print(f"  æ•°å­—æ–‡ä»¶åæ¯”ä¾‹: {df['is_numeric'].mean()*100:.1f}%")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°CSV
    report_file = "image_analysis_report.csv"
    df.to_csv(report_file, index=False, encoding='utf-8-sig')
    print(f"  è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    # 9. ç”ŸæˆJSONå’ŒMarkdownè¯¦ç»†æŠ¥å‘Š
    print("\nğŸ“„ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š:")
    print("-" * 30)
    
    # ç”ŸæˆJSONæŠ¥å‘Š
    json_report = generate_json_report(analysis_data)
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    markdown_report = generate_markdown_report(analysis_data, json_report)
    
    print("=" * 60)
    print("åˆ†æå®Œæˆï¼")
    print(f"ğŸ“Š å·²ç”Ÿæˆä»¥ä¸‹æŠ¥å‘Šæ–‡ä»¶:")
    print(f"  - detailed_image_analysis.json (ç»“æ„åŒ–æ•°æ®)")
    print(f"  - detailed_image_analysis.md (æ ¼å¼åŒ–æŠ¥å‘Š)")
    print(f"  - image_analysis_report.csv (è¯¦ç»†æ•°æ®)")
    print("=" * 60)

if __name__ == "__main__":
    analyze_images_directory()
